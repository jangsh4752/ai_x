정적 웹데이터 수집 (~/robots.txt에 해당 사이트 웹크롤링 허용 범위 표기)
방법1 : request모듈 이용
headers = {'user-agent':'user-agent내용(브라우저에서 복사)'}
response = reqiests.get(url, headers=headers) # url에 한글, 특수문자(' ')
soup = BeautifulSoup(response.text, 'html.parser')

방법2 : urllib.request모듈 이용
url = urllib.parse.quote('한글부분') 이용
request = urllib.request.Request(url, headers=headers)
response = urllib.request.urlopen(url)
soup = BeautifulSoup(response.text, 'html.parser')

=>soup.select('선택자')나 soup.select_one('선택자')나
    soup.fine('태그', 속성(dict, class_)나
    soup.find_all('태그들 list','속성')